# KNIME â€” Scraping de precio del azufre y extracciÃ³n/clasificaciÃ³n de noticias

---

## ğŸ§­ Objetivo

Construir un workflow **KNIME** que:

1. Obtenga y normalice **series de precios del azufre** (fuentes pÃºblicas/API o scraping permitido).
2. Ingesta y clasifique **noticias** relevantes (agro/fertilizantes) sobre â€œsulfur/sulfuro/azufreâ€.
3. Exporte datasets limpios (CSV/JSON) listos para **Power BI** / anÃ¡lisis.

---

## ğŸ“ Estructura del repo

```
knime-sulfur-news/
â”œâ”€ README.md                    
â”œâ”€ LICENSE                       
â”œâ”€ .gitignore
â”œâ”€ /workflows/
â”‚   â”œâ”€ sulfur_prices.knwf        
â”‚   â””â”€ sulfur_news.knwf          
â”œâ”€ /data/
â”‚   â”œâ”€ raw/                     
â”‚   â”œâ”€ interim/                 
â”‚   â””â”€ processed/              
â”œâ”€ /docs/
â”‚   â”œâ”€ schema_prices.json      
â”‚   â”œâ”€ schema_news.json          
â”‚   â””â”€ screenshots/              
â””â”€ /notebooks/                  
```

---

## âš™ï¸ Requisitos

* **KNIME Analytics Platform** â‰¥ 5.x

---

## ğŸ”Œ Fuentes de datos (ejemplos)

### Precios

* SunSirs: URL/API principal para precios (p.ej. proveedor pÃºblico, Ã­ndice spot, base propia).
* Formato recomendado de salida (**schema_prices**):

```json
{
  "date": "YYYY-MM-DD",
  "price_usd_t": 123.45,
  "source": "<string>",
  "frequency": "daily|weekly|monthly",
  "notes": "<string opcional>"
}
```

### Noticias (GDELT v2, ejemplo)

* Endpoint: `https://api.gdeltproject.org/api/v2/doc/doc?query=<QUERY>&mode=artlist&maxrecords=250&format=json`
* `QUERY` sugerida (agro/fertilizantes):

```
("sulfur" OR "sulphur" OR "azufre") AND (fertilizer OR fertiliser OR agriculture OR agro)
```

* Campos mÃ­nimos en salida (**schema_news**):

```json
{
  "date": "YYYY-MM-DD",
  "title": "<string>",
  "url": "<string>",
  "lang": "en|es|...",
  "source": "<domain>",
  "summary": "<string>",
  "tags": ["agriculture", "fertilizer"],
  "relevance": 0.0
}
```

> **Nota:** Respeta robots.txt y tÃ©rminos de uso de cualquier web. Prioriza APIs y feeds.

---

## ğŸ› ï¸ Workflows KNIME 

### 1) `sulfur_prices.knwf`

**Bloques:**

1. **Web Interaction Start (Labs)** â†’ inicializa la sesiÃ³n de navegador (Selenium) para poder renderizar la pÃ¡gina de SunSirs con JS.
2. **Navigator (Labs)** â†’ navega a la URL objetivo del azufre en SunSirs y espera la carga completa del contenido.
3. **Content Retriever (Labs)** â†’ captura el **HTML renderizado** (no sÃ³lo la fuente estÃ¡tica) para que el siguiente nodo pueda parsearlo.
4. **XPath** â†’ extrae **fecha** y **precio** del HTML a columnas tabulares (`date_raw`, `price_raw`).
5. **String to Number** â†’ convierte `price_raw` a tipo numÃ©rico (double), eliminando sÃ­mbolos y separadores.
6. **Column Filter** â†’ conserva Ãºnicamente las columnas necesarias (fecha, precio y/o fuente) y descarta ruido.
7. **String Manipulation** â†’ limpia la fecha (trim, reemplazo de espacios no-break) y la normaliza a formato `yyyy-MM-dd` en una columna `date_clean`.
8. **String to Date&Time** â†’ convierte `date_clean` a tipo **Local Date** (`date`).
9. **Column Resorter** â†’ reordena columnas finales en el orden lÃ³gico: `date`, `price`, `source`.
10. **CSV Reader** â†’ carga el **histÃ³rico previo** (`data/processed/sulfur_prices.csv`) para evitar duplicados cuando se aÃ±aden nuevas filas.
11. **String to Date&Time** (histÃ³rico) â†’ asegura que la columna de fecha del histÃ³rico tambiÃ©n sea **Local Date** y comparable.
12. **Reference Row Filter** â†’ deja **solo los registros nuevos** comparando por `date` frente al histÃ³rico (evita duplicados).
13. **CSV Writer** â†’ exporta/actualiza la serie consolidada en `/data/processed/sulfur_prices.csv` (append o overwrite segÃºn configuraciÃ³n).


### 2) `sulfur_news.knwf`

**Bloques:**

1. **Web Interaction Start (Labs)** â†’ inicializa la sesiÃ³n del navegador (Selenium) para habilitar la carga dinÃ¡mica de los portales de noticias relacionados con el azufre.
2. **Navigator (Labs)** â†’ abre la pÃ¡gina de listados de artÃ­culos (por ejemplo, en SunSirs u otras fuentes de fertilizantes) y espera a que el contenido HTML se renderice completamente.
3. **Content Retriever (Labs)** â†’ obtiene el cÃ³digo HTML completo del listado de noticias para poder analizarlo mediante expresiones XPath.
4. **XPath** â†’ extrae los **tÃ­tulos**, **enlaces (URLs)** y **fechas** de publicaciÃ³n de los artÃ­culos.
5. **String Manipulation** â†’ elimina espacios en blanco, saltos de lÃ­nea o caracteres no imprimibles en los textos.
6. **String Manipulation (2)** â†’ normaliza el formato de las fechas (`yyyy-MM-dd`) y asegura que todas las URLs comiencen con `https://`.
7. **Web Interaction Start (Labs)** (segunda rama) â†’ inicia una nueva sesiÃ³n Selenium para navegar a las pÃ¡ginas individuales de las noticias.
8. **Navigator (Labs)** (2) â†’ recorre cada URL obtenida para capturar el texto completo de las noticias y los posibles resÃºmenes.
9. **Content Retriever (Labs)** (2) â†’ descarga el HTML renderizado de cada noticia individual.
10. **XPath** (2) â†’ extrae el cuerpo principal del texto o el resumen de cada noticia.
11. **XPath** (3)** â†’ recoge informaciÃ³n adicional (fuente, categorÃ­a o etiquetas del artÃ­culo).
12. **Duplicate Row Filter** â†’ elimina duplicados basÃ¡ndose en `url` o la combinaciÃ³n `title + date`.
13. **Row Filter** â†’ mantiene Ãºnicamente las noticias relevantes sobre azufre en el contexto **agrÃ­cola/fertilizante**, filtrando por palabras clave (`sulfur`, `azufre`, `fertilizer`, `agriculture`).
14. **Column Filter** â†’ conserva solo las columnas finales necesarias (`date`, `title`, `url`, `source`, `summary`).
15. **Concatenate** â†’ une los datos procedentes de las dos ramas (listado y detalle) en una tabla final consolidada.
16. **CSV Writer** â†’ exporta el conjunto de noticias limpio y deduplicado a `/data/processed/sulfur_news.csv` (o `.json` si se desea en formato JSON).


**Tips Ãºtiles**

* Para **espacios en blanco** tras `XPath/JSON`: usa **String Manipulation** con `strip($col$)` o **Column Expressions**: `replaceChars(column("title"), "\u00A0", " ")` y `strip()`.
* Si una pÃ¡gina devuelve **missing**, valida cabeceras con **GET Request** (User-Agent) y maneja cÃ³digos 429/403 con **Wait...** + **Retry**.
* Logs: aÃ±ade **Table Writer** a `/data/interim/log_runs.csv` con marca temporal.


### 3) Python Script vaderSentiment (opcional)

```
import pandas as pd
import numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

df = input_table_1.copy()

analyzer = SentimentIntensityAnalyzer()

def score_text(t):
    if not isinstance(t, str) or not t.strip():
        return {"neg": np.nan, "neu": np.nan, "pos": np.nan, "compound": np.nan}
    return analyzer.polarity_scores(t)

scores = df["body"].apply(score_text).apply(pd.Series)
df["vader_neg"] = scores["neg"]
df["vader_neu"] = scores["neu"]
df["vader_pos"] = scores["pos"]
df["vader_compound"] = scores["compound"]

def label(c):
    if pd.isna(c): return None
    if c >= 0.05:  return "positive"
    if c <= -0.05: return "negative"
    return "neutral"

df["sentiment_label"] = df["vader_compound"].apply(label)

output_table_1 = df
```

---

