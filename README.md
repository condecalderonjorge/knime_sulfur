# KNIME â€” Scraping de precio del azufre y extracciÃ³n/clasificaciÃ³n de noticias

---

## ğŸ§­ Objetivo

Construir un workflow **KNIME** que:

1. Obtenga y normalice **series de precios del azufre** (fuentes pÃºblicas/API o scraping permitido).
2. Ingesta y clasifique **noticias** relevantes (agro/fertilizantes) sobre â€œsulfur/sulfuro/azufreâ€.
3. Exporte datasets limpios (CSV/JSON) listos para **Power BI** / anÃ¡lisis.

---

## ğŸ“ Estructura del repo

```
knime-sulfur-news/
â”œâ”€ README.md                     # Este archivo
â”œâ”€ LICENSE                       # MIT por defecto (opcional)
â”œâ”€ .gitignore
â”œâ”€ .gitattributes                # (opcional) Git LFS para ficheros grandes
â”œâ”€ /workflows/
â”‚   â”œâ”€ sulfur_prices.knwf        # Workflow KNIME precios 
â”‚   â””â”€ sulfur_news.knwf          # Workflow KNIME noticias 
â”œâ”€ /data/
â”‚   â”œâ”€ raw/                      # Descargas originales (no versionar si son pesadas)
â”‚   â”œâ”€ interim/                  # Limpiezas intermedias
â”‚   â””â”€ processed/                # Salidas finales (CSV/Parquet/JSON)
â”œâ”€ /docs/
â”‚   â”œâ”€ schema_prices.json        # Esquema de salida precios
â”‚   â”œâ”€ schema_news.json          # Esquema de salida noticias
â”‚   â””â”€ screenshots/              # Capturas de workflows / ejemplos
â””â”€ /notebooks/                   # (opcional) Validaciones en Python/R
```

---

## âš™ï¸ Requisitos

* **KNIME Analytics Platform** â‰¥ 5.x

---

## ğŸ”Œ Fuentes de datos (ejemplos)

### Precios

* `TODO:` URL/API principal para precios (p.ej. proveedor pÃºblico, Ã­ndice spot, base propia).
* Formato recomendado de salida (**schema_prices**):

```json
{
  "date": "YYYY-MM-DD",
  "price_usd_t": 123.45,
  "source": "<string>",
  "frequency": "daily|weekly|monthly",
  "notes": "<string opcional>"
}
```

### Noticias (GDELT v2, ejemplo)

* Endpoint: `https://api.gdeltproject.org/api/v2/doc/doc?query=<QUERY>&mode=artlist&maxrecords=250&format=json`
* `QUERY` sugerida (agro/fertilizantes):

```
("sulfur" OR "sulphur" OR "azufre") AND (fertilizer OR fertiliser OR agriculture OR agro)
```

* Campos mÃ­nimos en salida (**schema_news**):

```json
{
  "date": "YYYY-MM-DD",
  "title": "<string>",
  "url": "<string>",
  "lang": "en|es|...",
  "source": "<domain>",
  "summary": "<string>",
  "tags": ["agriculture", "fertilizer"],
  "relevance": 0.0
}
```

> **Nota:** Respeta robots.txt y tÃ©rminos de uso de cualquier web. Prioriza APIs y feeds.

---

## ğŸ› ï¸ Workflows KNIME 

### 1) `sulfur_prices.knwf`

**Bloques:**

1. **HTTP Retriever / CSV Reader / Excel Reader** â†’ ingesta desde API/archivo.
2. **String/Column Manipulation** â†’ limpieza (trim, lower, parseo numÃ©rico).
3. **Date&Time** â†’ normalizaciÃ³n de fecha a `YYYY-MM-DD` y *timezone*.
4. **Missing Value** â†’ imputaciÃ³n/descartes.
5. **Rule Engine** â†’ filtrar anÃ³malos (negativos, outliers evidentes).
6. **GroupBy / Window** â†’ agregaciones (W/M) si procede.
7. **Column Rename** â†’ conformar esquema final.
8. **CSV/Parquet Writer** â†’ `/data/processed/sulfur_prices.csv`.

### 2) `sulfur_news.knwf`

**Bloques:**

1. **GET Request** â†’ GDELT (`mode=artlist`, `format=json`, `maxrecords` controlado).
2. **JSON to Table** / **JSON Path** â†’ expandir array `articles`.
3. **String Manipulation (Multi Column)** â†’ `strip()` espacios, normalizar idioma.
4. **Regex Extractor** â†’ dominio de `url` â†’ `source`.
5. **Row Filter** â†’ incluir solo agro/fertilizantes (consulta o palabras clave secundarias).
6. **Duplicate Row Filter** â†’ por `url` o `title+date`.
7. **Rule Engine** â†’ *scoring* sencillo de relevancia por tÃ©rminos (p.ej. +1 â€œfertilizerâ€, +1 â€œagricultureâ€, -1 â€œbatteryâ€).
8. **CSV/JSON Writer** â†’ `/data/processed/sulfur_news.json` y `.csv`.

**Tips Ãºtiles**

* Para **espacios en blanco** tras `XPath/JSON`: usa **String Manipulation** con `strip($col$)` o **Column Expressions**: `replaceChars(column("title"), "\u00A0", " ")` y `strip()`.
* Si una pÃ¡gina devuelve **missing**, valida cabeceras con **GET Request** (User-Agent) y maneja cÃ³digos 429/403 con **Wait...** + **Retry**.
* Logs: aÃ±ade **Table Writer** a `/data/interim/log_runs.csv` con marca temporal.

---

