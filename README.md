# KNIME ‚Äî Scraping de precio del azufre y extracci√≥n/clasificaci√≥n de noticias

---

## üß≠ Objetivo

Construir un workflow **KNIME** que:

1. Obtenga y normalice **series de precios del azufre** (fuentes p√∫blicas/API o scraping permitido).
2. Ingesta y clasifique **noticias** relevantes (agro/fertilizantes) sobre ‚Äúsulfur/sulfuro/azufre‚Äù.
3. Exporte datasets limpios (CSV/JSON) listos para **Power BI** / an√°lisis.

---

## üß± Estructura del proyecto
| Componente | Descripci√≥n |
|-------------|-------------|
| `Reto Buconda.knwf` | Workflow de scraping de precios y noticias. |
| `Workflow knime.png` | Vista general del workflow. |

---

## ‚öôÔ∏è Requisitos

* **KNIME Analytics Platform** ‚â• 5.x

---

## üîå Fuentes de datos (ejemplos)

### Precios

* SunSirs: URL/API principal para precios (p.ej. proveedor p√∫blico, √≠ndice spot, base propia).
* Formato recomendado de salida (**schema_prices**):

```json
{
  "date": "YYYY-MM-DD",
  "price_usd_t": 123.45,
  "source": "<string>",
  "frequency": "daily|weekly|monthly",
  "notes": "<string opcional>"
}
```

### Noticias (GDELT v2, ejemplo)

* Endpoint: `https://api.gdeltproject.org/api/v2/doc/doc?query=<QUERY>&mode=artlist&maxrecords=250&format=json`
* `QUERY` sugerida (agro/fertilizantes):

```
("sulfur" OR "sulphur" OR "azufre") AND (fertilizer OR fertiliser OR agriculture OR agro)
```

* Campos m√≠nimos en salida (**schema_news**):

```json
{
  "date": "YYYY-MM-DD",
  "title": "<string>",
  "url": "<string>",
  "lang": "en|es|...",
  "source": "<domain>",
  "summary": "<string>",
  "tags": ["agriculture", "fertilizer"],
  "relevance": 0.0
}
```

> **Nota:** Respeta robots.txt y t√©rminos de uso de cualquier web. Prioriza APIs y feeds.

---

## üõ†Ô∏è Workflows KNIME 

### 1) `sulfur_prices.knwf`

**Bloques:**

1. **Web Interaction Start (Labs)** ‚Üí inicializa la sesi√≥n de navegador (Selenium) para poder renderizar la p√°gina de SunSirs con JS.
2. **Navigator (Labs)** ‚Üí navega a la URL objetivo del azufre en SunSirs y espera la carga completa del contenido.
3. **Content Retriever (Labs)** ‚Üí captura el **HTML renderizado** (no s√≥lo la fuente est√°tica) para que el siguiente nodo pueda parsearlo.
4. **XPath** ‚Üí extrae **fecha** y **precio** del HTML a columnas tabulares (`date_raw`, `price_raw`).
5. **String to Number** ‚Üí convierte `price_raw` a tipo num√©rico (double), eliminando s√≠mbolos y separadores.
6. **Column Filter** ‚Üí conserva √∫nicamente las columnas necesarias (fecha, precio y/o fuente) y descarta ruido.
7. **String Manipulation** ‚Üí limpia la fecha (trim, reemplazo de espacios no-break) y la normaliza a formato `yyyy-MM-dd` en una columna `date_clean`.
8. **String to Date&Time** ‚Üí convierte `date_clean` a tipo **Local Date** (`date`).
9. **Column Resorter** ‚Üí reordena columnas finales en el orden l√≥gico: `date`, `price`, `source`.
10. **CSV Reader** ‚Üí carga el **hist√≥rico previo** (`data/processed/sulfur_prices.csv`) para evitar duplicados cuando se a√±aden nuevas filas.
11. **String to Date&Time** (hist√≥rico) ‚Üí asegura que la columna de fecha del hist√≥rico tambi√©n sea **Local Date** y comparable.
12. **Reference Row Filter** ‚Üí deja **solo los registros nuevos** comparando por `date` frente al hist√≥rico (evita duplicados).
13. **CSV Writer** ‚Üí exporta/actualiza la serie consolidada en `/data/processed/sulfur_prices.csv` (append o overwrite seg√∫n configuraci√≥n).


### 2) `sulfur_news.knwf`

**Bloques:**

1. **Web Interaction Start (Labs)** ‚Üí inicializa la sesi√≥n del navegador (Selenium) para habilitar la carga din√°mica de los portales de noticias relacionados con el azufre.
2. **Navigator (Labs)** ‚Üí abre la p√°gina de listados de art√≠culos (por ejemplo, en SunSirs u otras fuentes de fertilizantes) y espera a que el contenido HTML se renderice completamente.
3. **Content Retriever (Labs)** ‚Üí obtiene el c√≥digo HTML completo del listado de noticias para poder analizarlo mediante expresiones XPath.
4. **XPath** ‚Üí extrae los **t√≠tulos**, **enlaces (URLs)** y **fechas** de publicaci√≥n de los art√≠culos.
5. **String Manipulation** ‚Üí elimina espacios en blanco, saltos de l√≠nea o caracteres no imprimibles en los textos.
6. **String Manipulation (2)** ‚Üí normaliza el formato de las fechas (`yyyy-MM-dd`) y asegura que todas las URLs comiencen con `https://`.
7. **Web Interaction Start (Labs)** (segunda rama) ‚Üí inicia una nueva sesi√≥n Selenium para navegar a las p√°ginas individuales de las noticias.
8. **Navigator (Labs)** (2) ‚Üí recorre cada URL obtenida para capturar el texto completo de las noticias y los posibles res√∫menes.
9. **Content Retriever (Labs)** (2) ‚Üí descarga el HTML renderizado de cada noticia individual.
10. **XPath** (2) ‚Üí extrae el cuerpo principal del texto o el resumen de cada noticia.
11. **XPath** (3)** ‚Üí recoge informaci√≥n adicional (fuente, categor√≠a o etiquetas del art√≠culo).
12. **Duplicate Row Filter** ‚Üí elimina duplicados bas√°ndose en `url` o la combinaci√≥n `title + date`.
13. **Row Filter** ‚Üí mantiene √∫nicamente las noticias relevantes sobre azufre en el contexto **agr√≠cola/fertilizante**, filtrando por palabras clave (`sulfur`, `azufre`, `fertilizer`, `agriculture`).
14. **Column Filter** ‚Üí conserva solo las columnas finales necesarias (`date`, `title`, `url`, `source`, `summary`).
15. **Concatenate** ‚Üí une los datos procedentes de las dos ramas (listado y detalle) en una tabla final consolidada.
16. **CSV Writer** ‚Üí exporta el conjunto de noticias limpio y deduplicado a `/data/processed/sulfur_news.csv` (o `.json` si se desea en formato JSON).


**Tips √∫tiles**

* Para **espacios en blanco** tras `XPath/JSON`: usa **String Manipulation** con `strip($col$)` o **Column Expressions**: `replaceChars(column("title"), "\u00A0", " ")` y `strip()`.
* Si una p√°gina devuelve **missing**, valida cabeceras con **GET Request** (User-Agent) y maneja c√≥digos 429/403 con **Wait...** + **Retry**.
* Logs: a√±ade **Table Writer** a `/data/interim/log_runs.csv` con marca temporal.


### 3) Python Script vaderSentiment (opcional)

```
import pandas as pd
import numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

df = input_table_1.copy()

analyzer = SentimentIntensityAnalyzer()

def score_text(t):
    if not isinstance(t, str) or not t.strip():
        return {"neg": np.nan, "neu": np.nan, "pos": np.nan, "compound": np.nan}
    return analyzer.polarity_scores(t)

scores = df["body"].apply(score_text).apply(pd.Series)
df["vader_neg"] = scores["neg"]
df["vader_neu"] = scores["neu"]
df["vader_pos"] = scores["pos"]
df["vader_compound"] = scores["compound"]

def label(c):
    if pd.isna(c): return None
    if c >= 0.05:  return "positive"
    if c <= -0.05: return "negative"
    return "neutral"

df["sentiment_label"] = df["vader_compound"].apply(label)

output_table_1 = df
```

---

